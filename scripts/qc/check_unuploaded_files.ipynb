{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check unuploaded files\n",
    "\n",
    "Three possible checks:\n",
    "\n",
    "- `no_matching_records`: Files that should have a matching import record, but don't\n",
    "- `matching_records_blank`: Files that have a matching import record, but no data on the corresponding form (happens when another processed file has been uploaded to the record, but not this file)\n",
    "- `orphaned_records`: Records with an ID that isn't matched by any file. (Sanity check.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "form = None\n",
    "target = None\n",
    "output_dir = None\n",
    "# event = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters-eval"
    ]
   },
   "outputs": [],
   "source": [
    "if target is not None:\n",
    "    assert target in [\"no_matching_records\", \"matching_records_blank\", \"orphaned_records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import redcap as rc\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/sibis-software/python-packages/')\n",
    "import sibispy\n",
    "from sibispy import sibislogger as slog\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sibispy.Session()\n",
    "if not session.configure():\n",
    "    sys.exit()\n",
    "\n",
    "slog.init_log(None, None, \n",
    "              'QC: Check all harvester-prepared CSVs are uploaded', \n",
    "              'check_unuploaded_files', None)\n",
    "slog.startTimer1()\n",
    "\n",
    "# Setting specific constants for this run of QC\n",
    "api = session.connect_server('import_laptops', True)\n",
    "primary_key = api.def_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = api.export_metadata(format_type='df')\n",
    "form_names = meta.form_name.unique().tolist()\n",
    "if form is not None:\n",
    "    # FIXME: This is incorrect - needs to reflect the short_to_long, etc.\n",
    "    if not form in form_names:\n",
    "        raise KeyError(\"{} not among Import Project forms\".format(form))\n",
    "    form_names_subset = [form]\n",
    "else:\n",
    "    form_names_subset = form_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taken from http://pycap.readthedocs.io/en/latest/deep.html#dealing-with-large-exports\n",
    "# # and adapted to scope down to forms\n",
    "# def chunked_export(project, form, chunk_size=100, verbose=True):\n",
    "#     def chunks(l, n):\n",
    "#         \"\"\"Yield successive n-sized chunks from list l\"\"\"\n",
    "#         for i in range(0, len(l), n):\n",
    "#             yield l[i:i+n]\n",
    "#     record_list = project.export_records(fields=[project.def_field])\n",
    "#     records = [r[project.def_field] for r in record_list]\n",
    "#     #print \"Total records: %d\" % len(records)\n",
    "#     try:\n",
    "#         response = None\n",
    "#         record_count = 0\n",
    "#         for record_chunk in chunks(records, chunk_size):\n",
    "#             record_count = record_count + chunk_size\n",
    "#             #print record_count\n",
    "#             chunked_response = project.export_records(records=record_chunk, \n",
    "#                                                       fields=[project.def_field],\n",
    "#                                                       forms=[form], \n",
    "#                                                       format_type='df',\n",
    "#                                                       df_kwargs={'low_memory': False})\n",
    "#             if response is not None:\n",
    "#                 response = pd.concat([response, chunked_response], axis=0)\n",
    "#             else:\n",
    "#                 response = chunked_response\n",
    "#     except rc.RedcapError:\n",
    "#         msg = \"Chunked export failed for chunk_size={:d}\".format(chunk_size)\n",
    "#         raise ValueError(msg)\n",
    "#     else:\n",
    "#         return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_form(api, form_name, verbose=True):\n",
    "#     if verbose:\n",
    "#         print(form_name)\n",
    "    \n",
    "#     # 1. Standard load attempt\n",
    "#     # try:\n",
    "#     #     print \"Trying standard export\"\n",
    "#     #     return api.export_records(fields=[api.def_field],\n",
    "#     #                               forms=[form_name],\n",
    "#     #                               format_type='df',\n",
    "#     #                               df_kwargs={'low_memory': False})\n",
    "#     # except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "#     #     pass\n",
    "#     try:\n",
    "#         print(\"Trying chunked export, 5000 records at a time\")\n",
    "#         return chunked_export(api, form_name, 5000)\n",
    "#     except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "#         pass\n",
    "    \n",
    "#     # 2. Chunked load with chunk size of 1000\n",
    "#     try:\n",
    "#         print(\"Trying chunked export, 1000 records at a time\")\n",
    "#         return chunked_export(api, form_name, 1000)\n",
    "#     except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "#         pass\n",
    "    \n",
    "#     # 2. Chunked load with default chunk size\n",
    "#     try:\n",
    "#         print(\"Trying chunked export, default chunk size (100)\")\n",
    "#         return chunked_export(api, form_name, 100)\n",
    "#     except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "#         pass\n",
    "    \n",
    "#     # 3. Chunked load with tiny chunk\n",
    "#     try:\n",
    "#         print(\"Trying chunked export with tiny chunks (10)\")\n",
    "#         return chunked_export(api, form_name, 10)\n",
    "#     except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "#         print(\"Giving up\")\n",
    "#         return None\n",
    "\n",
    "# def load_form_with_primary_key(api, form_name, verbose=True):\n",
    "#     df = load_form(api, form_name, verbose)\n",
    "#     if df is not None:\n",
    "#         return df.set_index(api.def_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_utils import load_form_with_primary_key\n",
    "all_data = {form_name: load_form_with_primary_key(api, form_name) for form_name in form_names_subset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract emptiness statistic from Import records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_nan_rowwise(df, form_name=None, drop_column=None):\n",
    "    \"\"\" A more efficient method of checking non-NaN values \"\"\"\n",
    "    # 1. check complete\n",
    "    if form_name:\n",
    "        complete_field = form_name + '_complete'\n",
    "        if drop_columns:\n",
    "            drop_columns.append(complete_field)\n",
    "        else:\n",
    "            drop_columns = [complete_field]\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "    \n",
    "    # 2. count up NaNs\n",
    "    return df.drop(drop_columns, axis=1).notnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to DF to get all empty records\n",
    "def set_emptiness_flags(row, form_name, drop_columns=None):\n",
    "    # 1. check complete\n",
    "    complete_field = form_name + '_complete'\n",
    "    #is_incomplete = row[complete_field] == 0  # TODO: maybe complete_field not in [1, 2] to catch NaNs?\n",
    "    \n",
    "    # 2. count up NaNs\n",
    "    if drop_columns:\n",
    "        drop_columns.append(complete_field)\n",
    "    else:\n",
    "        drop_columns = [complete_field]\n",
    "    # NOTE: This will only work for a Series\n",
    "    # NOTE: For a full Data Frame, use df.drop(drop_columns, axis=1).notnull().sum(axis=1)\n",
    "    non_nan_count = row.drop(drop_columns).notnull().sum()\n",
    "    \n",
    "    return pd.Series({'completion_status': row[complete_field], 'non_nan_count': non_nan_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptiness_df = {form_name: all_data[form_name].apply(lambda x: set_emptiness_flags(x, form_name), axis=1) \n",
    "                for form_name in all_data.keys() \n",
    "                if all_data[form_name] is not None}\n",
    "#all_data['recovery_questionnaire'].apply(lambda x: set_emptiness_flags(x, 'recovery_questionnaire'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for form_name in emptiness_df.keys():\n",
    "    emptiness_df[form_name]['form'] = form_name\n",
    "all_forms_emptiness = pd.concat(emptiness_df.values())\n",
    "all_forms_emptiness.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_to_long = {\n",
    "    # Forms for Arm 1: Standard Protocol  \n",
    "    'dd100': 'delayed_discounting_100',\n",
    "    'dd1000': 'delayed_discounting_1000',\n",
    "\n",
    "    'pasat': 'paced_auditory_serial_addition_test_pasat',\n",
    "    'stroop': 'stroop',\n",
    "    \n",
    "    'ssaga_youth': 'ssaga_youth',\n",
    "    'ssaga_parent': 'ssaga_parent',\n",
    "    'youthreport1': 'youth_report_1',\n",
    "    'youthreport1b': 'youth_report_1b',\n",
    "    'youthreport2': 'youth_report_2',\n",
    "    'parentreport': 'parent_report',\n",
    "    \n",
    "    'mrireport': 'mri_report',\n",
    "    'plus': 'participant_last_use_summary',\n",
    "    \n",
    "    'myy': 'midyear_youth_interview',\n",
    "    \n",
    "    'lssaga1_youth': 'limesurvey_ssaga_part_1_youth',\n",
    "    'lssaga2_youth': 'limesurvey_ssaga_part_2_youth',\n",
    "    'lssaga3_youth': 'limesurvey_ssaga_part_3_youth',\n",
    "    'lssaga4_youth': 'limesurvey_ssaga_part_4_youth',\n",
    "    \n",
    "    'lssaga1_parent': 'limesurvey_ssaga_part_1_parent',\n",
    "    'lssaga2_parent': 'limesurvey_ssaga_part_2_parent',\n",
    "    'lssaga3_parent': 'limesurvey_ssaga_part_3_parent',\n",
    "    'lssaga4_parent': 'limesurvey_ssaga_part_4_parent',\n",
    "\n",
    "    # Forms for Arm 3: Sleep Studies\n",
    "    'sleepeve': 'sleep_study_evening_questionnaire',\n",
    "    'sleeppre': 'sleep_study_presleep_questionnaire',\n",
    "    'sleepmor': 'sleep_study_morning_questionnaire',\n",
    "\n",
    "    # Forms for Recovery project\n",
    "    'recq': 'recovery_questionnaire',\n",
    "    \n",
    "    # Forms for UCSD\n",
    "    'parent': 'ssaga_parent',\n",
    "    'youth': 'ssaga_youth',\n",
    "    'deldisc': 'delayed_discounting'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.DataFrame(columns=[\"file\", \"path\", \"form\"])\n",
    "records = []\n",
    "record_paths = []\n",
    "for root, subdirs, files in os.walk('/fs/storage/laptops/imported'):\n",
    "    csv_files = [f for f in files if (f.endswith(\".csv\") and not f.endswith(\"-fields.csv\"))]\n",
    "    if csv_files:\n",
    "        folder_df = pd.DataFrame(columns=[\"file\", \"path\", \"form\"])\n",
    "        folder_df['file'] = csv_files\n",
    "        folder_df['path'] = [root + \"/\" + f for f in csv_files]\n",
    "        \n",
    "        root_parts = root.split('/')\n",
    "        current_folder = root_parts[-1]\n",
    "        try:\n",
    "            form = short_to_long[current_folder]\n",
    "            if form not in form_names_subset:\n",
    "                continue\n",
    "            else:\n",
    "                folder_df['form'] = form\n",
    "                files_df = pd.concat([files_df, folder_df])\n",
    "            \n",
    "        except KeyError as e:\n",
    "            continue\n",
    "files_df.set_index(\"path\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecordIDFromFile(row):\n",
    "    import re\n",
    "    bare_file = re.sub(r\"\\.csv$\", \"\", row[\"file\"])\n",
    "    bare_file = re.sub(r\"^\\s+|\\s+$\", \"\", bare_file)\n",
    "    if row[\"form\"] == \"delayed_discounting\":\n",
    "        bare_file = re.sub(\"-1000?$\", \"\", bare_file)\n",
    "    return bare_file\n",
    "files_df[\"record_id\"] = files_df.apply(getRecordIDFromFile, axis=1)\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixFormName(row):\n",
    "    import re\n",
    "    if row[\"form\"] == \"delayed_discounting\":\n",
    "        if re.search(r\"-100\\.csv$\", row[\"file\"]):\n",
    "            return \"delayed_discounting_100\"\n",
    "        elif re.search(r\"-1000\\.csv$\", row[\"file\"]):\n",
    "            return \"delayed_discounting_1000\"\n",
    "        else:\n",
    "            return \"delayed_discounting\"\n",
    "    else:\n",
    "        return row[\"form\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df[\"form\"] = files_df.apply(fixFormName, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap = pd.merge(files_df.reset_index(),\n",
    "                           all_forms_emptiness.reset_index(), \n",
    "                           on=[\"record_id\", \"form\"], \n",
    "                           how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap.head()\n",
    "if output_dir is not None:\n",
    "    files_in_redcap.to_csv(os.path.join(output_dir, \"all_files_upload_status.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get results\n",
    "## Processed files that weren't matched at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (target is None) or (target == \"no_matching_records\"):\n",
    "    unmatched_files = files_in_redcap.loc[files_in_redcap.completion_status.isnull()]\n",
    "    if output_dir is not None:\n",
    "        unmatched_files.to_csv(os.path.join(output_dir, \"no_matching_records.csv\"), index=False)\n",
    "    display(unmatched_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files that were matched but have blank forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_file_empty(row):\n",
    "    contents = pd.read_csv(row['path'])\n",
    "    return contents.dropna(axis=\"columns\").shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (target is None) or (target == \"matching_records_blank\"):\n",
    "    matched_blank_index = files_in_redcap['path'].notnull() & (files_in_redcap['non_nan_count'] == 0)\n",
    "    files_in_redcap.loc[matched_blank_index, 'file_value_count'] = (\n",
    "        files_in_redcap\n",
    "        .loc[matched_blank_index]\n",
    "        .apply(check_if_file_empty, axis=1))\n",
    "    matched_blank = files_in_redcap.loc[matched_blank_index & (files_in_redcap['file_value_count'] > 0)]\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        matched_blank.to_csv(os.path.join(output_dir, \"matched_blank.csv\"), index=False)\n",
    "    display(matched_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records that don't match harvested CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (target is None) or (target == \"orphaned_records\"):\n",
    "    orphaned_records = files_in_redcap.loc[files_in_redcap['path'].isnull() & (files_in_redcap['non_nan_count'] > 0)]\n",
    "    if (output_dir is not None) and (orphaned_records.shape[0] > 0):\n",
    "        orphaned_records.to_csv(os.path.join(output_dir, \"orphaned_records.csv\"), index=False)\n",
    "    display(orphaned_records)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
