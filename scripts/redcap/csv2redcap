#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

# Setup command line parser
import os
import re
import sys
import string
import hashlib
import argparse

import pandas

import redcap
import sibispy
from sibispy import sibislogger as slog

parser = argparse.ArgumentParser( description="Import contents of CSV file into longitudinal REDCap project" )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "-f", "--force", help="Force overwriting of existing records.", action="store_true")
parser.add_argument( "--project", help="REDCAP project (import_laptops, import_webcnp, or data_entry)", default="data_entry" )
parser.add_argument( "--data-access-group", help="REDCap project data access group that the imported data should be assigned to.", default=None )
parser.add_argument( "csvfile", help="Input .csv file.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")
parser.add_argument("-t", "--time-log-dir",
                        help="If set then time logs are written to that directory",
                        action="store",
                        default=None)

args = parser.parse_args()

slog.init_log(args.verbose, args.post_to_github, 'NCANDA REDCap', 'redcap-csv2redcap',
                  args.time_log_dir)
slog.startTimer1()

# Read input file
data = pandas.io.parsers.read_csv( args.csvfile )

# Replace periods in column labels with underscores
data.rename( columns = lambda s: string.lower( s.replace( '.', '_' ) ), inplace=True )

# Bring original "siteid" column back to assign each record to the correct data access group
if not args.data_access_group == None:
    data['redcap_data_access_group'] = args.data_access_group

# First REDCap connection for the Summary project (this is where we put data)
session = sibispy.Session()
if not session.configure():
    if args.verbose:
        print "Error: session configure file was not found"

    sys.exit()

# If connection to redcap server fail, try multiple times
try:
    project = session.connect_server(args.project, True)
except Exception, error:
    slog.info(hashlib.sha1('csv2redcap').hexdigest()[0:6],
    "ERROR: Could not connect to redcap!",
    script = 'csv2redcap')
    sys.exit()


# Get "complete" field of existing records so we can protect "Complete" records from overwriting
complete_field = ''
if not args.force:
    for var in data.columns:
        if re.match( '.*_complete$', var ):
            complete_field = var

# Is there a "complete" field? Then get existing records and ditch all records from new data that are "Complete"
existing_data=pandas.DataFrame
if complete_field != '':
    existing_data = project.export_records( fields=[complete_field], format='df', df_kwargs={"index_col":[project.def_field, "redcap_event_name"]} )

# Make list of dicts for REDCap import
record_list = []
for [key,row] in data.iterrows():
    redcap_key = ( row['study_id'], row['redcap_event_name'] )
    if complete_field != '' and redcap_key in existing_data.index.tolist():
        if existing_data[complete_field][redcap_key] == 2:
            continue
    if not args.force:
        record = dict( row.dropna() )
    else:
        record = dict( row.fillna('') )
    record_list.append( record )

# Upload new data to REDCap
import_response = project.import_records( record_list, overwrite='overwrite' )

# If there were any errors, try to print them as well as possible
if 'error' in import_response.keys():
    print "UPLOAD ERROR:", import_response['error']

if 'fields' in import_response.keys():
    for field in import_response['fields']:
        print "\t", field

if 'records' in import_response.keys():
    for record in import_response['records']:
        print "\t", record

# Finally, print upload status if so desired
if args.verbose and 'count' in import_response.keys():
    print "Successfully uploaded %d/%d records to REDCap." % ( import_response['count'], len( data ) )

slog.takeTimer1("script_time","{'records': " +  str(import_response['count']) + "}")
