#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

# Setup command line parser
from __future__ import print_function
from builtins import str
import os
import re
import sys
import string
import hashlib
import argparse

import pandas

import redcap
import sibispy
from sibispy import sibislogger as slog

parser = argparse.ArgumentParser( description="Import contents of CSV file into longitudinal REDCap project" )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "-f", "--force", help="Force overwriting of existing records.", action="store_true")
parser.add_argument( "--project", help="REDCAP project (import_laptops, import_webcnp, or data_entry)", default="data_entry" )
parser.add_argument( "--data-access-group", help="REDCap project data access group that the imported data should be assigned to.", default=None )
parser.add_argument( "csvfile", help="Input .csv file.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")
parser.add_argument("-t", "--time-log-dir",
                        help="If set then time logs are written to that directory",
                        action="store",
                        default=None)

args = parser.parse_args()

slog.init_log(args.verbose, args.post_to_github, 'NCANDA REDCap', 'redcap-csv2redcap',
                  args.time_log_dir)
slog.startTimer1()

# Read input file
data = pandas.io.parsers.read_csv( args.csvfile )

# Replace periods in column labels with underscores
data.rename( columns = lambda s: s.replace( '.', '_' ).lower(), inplace=True )

# Bring original "siteid" column back to assign each record to the correct data access group
if not args.data_access_group == None:
    data['redcap_data_access_group'] = args.data_access_group

# First REDCap connection for the Summary project (this is where we put data)
session = sibispy.Session()
if not session.configure():
    if args.verbose:
        print("Error: session configure file was not found")

    sys.exit()

# If connection to redcap server fail, try multiple times
try:
    project = session.connect_server(args.project, True)
except Exception as error:
    slog.info(hashlib.sha1(b'csv2redcap').hexdigest()[0:6],
    "ERROR: Could not connect to redcap!",
    script = 'csv2redcap')
    sys.exit()


# Get "complete" field of existing records so we can protect "Complete" records from overwriting
complete_field = ''
if not args.force:
    for var in data.columns:
        if re.match( '.*_complete$', var ):
            complete_field = var

# Is there a "complete" field? Then get existing records and ditch all records from new data that are "Complete"
existing_data=pandas.DataFrame
if complete_field != '':
    existing_data = project.export_records( fields=[complete_field], format_type='df', df_kwargs={"index_col":[project.def_field, "redcap_event_name"]} )

# Make list of dicts for REDCap import
record_list = []
for [key,row] in data.iterrows():
    redcap_key = ( row['study_id'], row['redcap_event_name'] )
    if complete_field != '' and redcap_key in existing_data.index.tolist():
        if existing_data[complete_field][redcap_key] == 2:
            continue
    if not args.force:
        record = dict( row.dropna() )
    else:
        record = dict( row.fillna('') )
    record_list.append( record )

# Upload new data to REDCap
import_response = project.import_records( record_list, overwrite='overwrite' )

# If there were any errors, try to print them as well as possible
if 'error' in list(import_response.keys()):
    print("UPLOAD ERROR:", import_response['error'])

if 'fields' in list(import_response.keys()):
    for field in import_response['fields']:
        print("\t", field)

if 'records' in list(import_response.keys()):
    for record in import_response['records']:
        print("\t", record)

# Finally, print upload status if so desired
if args.verbose and 'count' in list(import_response.keys()):
    print("Successfully uploaded %d/%d records to REDCap." % ( import_response['count'], len( data ) ))

slog.takeTimer1("script_time","{'records': " +  str(import_response['count']) + "}")
