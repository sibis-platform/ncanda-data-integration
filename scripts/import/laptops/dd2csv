#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

from __future__ import print_function
from builtins import str
import tempfile
import pandas
import re
import os


def post_issue_and_exit(issue_title, **kwargs):
    import convert_util
    convert_util.post_issue_and_exit('dd2csv', args.ddfile, args.verbose, args.post_to_github, subject_label, issue_title, **kwargs)


# Setup command line parser
import argparse
parser = argparse.ArgumentParser( description="Convert Delayed Discounting results file to CSV" )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "--overwrite", help="Overwrite existing CSV files.", action="store_true")
parser.add_argument( "ddfile", help="Input .txt file with Delayed Discounting results.")
parser.add_argument( "outdir", help="Output directory. All CSV files are created in this directory")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")
args = parser.parse_args()

#Get experiment_id for logs
subject_label = str(args.ddfile).split('/')[-2]

# Make a temporary directory
# temp_dir_path = tempfile.mkdtemp()

# Read the temporary CSV file
try: 
   dd = pandas.read_csv(args.ddfile, sep='\t', header=None, names=['SubjectID', 'FuturePresent', 'Reward', 'HypReal', 'Exp1', 'Exp2', 'Amount', 'Days', 'DelDisc', 'Version', 'Date'])
except Exception as err_msg:
    post_issue_and_exit('Error: could not load file  %s' % args.ddfile , err_msg = str(err_msg))
#
# Check correctness of file format 
#
dd = dd.dropna(axis=0, how='all')
if len( dd ) == 0:
    post_issue_and_exit('Error: file %s is empty or of the wrong format' % args.ddfile)

# Check if this file has the right number of rows.
dd = dd[ dd['FuturePresent'] == 'future' ]
dd = dd[ dd['Reward'] == 'money' ]
dd = dd[ dd['HypReal'] == 'hyp' ]
dd = dd[ dd['Exp1'] == 'gain' ]

# Anything still left? If not, this file has nothing for us.
if len( dd ) == 0:
    post_issue_and_exit('Error: no suitable data in file %s' % args.ddfile)

# Make sure all rows have the same "Amount", and it's either 100 or 1000
amount = dd['Amount'][0]
if not amount in [100,1000]:
    post_issue_and_exit('Error: amount %f is neither 100 nor 1000 in %s' % (amount, args.ddfile))

dd = dd[ dd['Amount'] == amount ]

#
# Prepare for import into redcap  
# 

# Now drop the columns we know we don't want anymore
dd = dd.drop(['FuturePresent', 'Reward', 'HypReal', 'Exp1', 'Exp2'], axis=1)

# Convert days to integer to avoid comparing floats
dd['Days'] = dd['Days'].map( int )


# Make proper subject ID
entered_id = dd['SubjectID'][0].upper()

# First, see if prefix is valid N-CANDA ID
match_id = re.search('^\s*([A-F]-[0-9]{5}-[MFT]-[0-9]).*', entered_id)
if match_id:
    subject_id = match_id.group(1)
else:
    # If that didn't work, truncate off "-1000" or "-100" suffix
    match_id = re.search('^\s*(.*)-1000?$', entered_id)
    if match_id:
        subject_id = match_id.group(1)
    else:
        # Still no luck, just use whatever the RAs entered
        subject_id = entered_id

# Bring "Date" into proper format
match_date = re.search('^([0-9]*)/([0-9]*)/(20[0-9]{2})$', dd['Date'][0])
if match_date:
    date = '%s-%02d-%02d' % (match_date.group(3), int(match_date.group(1)), int(match_date.group(2)))
else:
    post_issue_and_exit('ERROR: %s\nCannot extract date from "%s"' % ( args.ddfile, dd['Date'][0] ))

# Create unique record ID
record_id = '%s-%s' % (subject_id, date)

# Create output table
prefix = 'dd%s' %  amount
data = {'record_id': record_id,
        'visit_information_complete' : 1,
        ('delayed_discounting_%s_complete' % amount) : 1 }

for ( days, field ) in [ (1, '1d'), (7, '7d'), (30, '1mo'), (182, '6mo') ]:
    selection = dd[ dd['Days'] == days ]
    if len( selection ) > 0:
        data[ '%s_logk_%s' % (prefix,field) ] = selection['DelDisc'].tolist()[0]
    else:
        data[ '%s_logk_%s' % (prefix,field) ] = ''

out_df = pandas.DataFrame(data=data, index=[0])

# Determine directory name - create if it doesn't exist
if not os.path.exists(args.outdir):
    os.makedirs(args.outdir)

# Determine file name, only proceed if file does not exist already
filename = os.path.join(args.outdir, '%s-%s.csv' % (record_id, amount))
if not os.path.exists(filename) or args.overwrite:
    out_df.to_csv(filename, index=False)
    # Print filename so we can get a list of updated files by capturing stdout
    print(filename)

# Clean up - remove temp directory
# import shutil
# shutil.rmtree(temp_dir_path)
