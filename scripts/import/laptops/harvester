#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
from __future__ import print_function, absolute_import
from builtins import str
import re
import os
import yaml
import argparse
import subprocess

import sys
from tqdm import tqdm

from sibispy import sibislogger as slog
from sibispy.svn_util import SibisSvnException, UpdateActionTypes
import sibispy 

from config_utils import flatten_path_dict

import hashlib
updated_files = []

import datetime 
# Setup command line parser
parser = argparse.ArgumentParser(description="Harvest incoming data files from "
                                             "SVN repository, call correct converter "
                                             "programs, and upload generated CSV files "
                                             "to REDCap")
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("--include-testing",
                    help="Include 'testing' data (as marked by a 'T' instead of a gender"
                         " code in the subject ID). Currently for single-session files "
                         "only.",
                    action="store_true")
parser.add_argument("--overwrite",
                    help="Overwrite existing CSV files.",
                    action="store_true")
parser.add_argument("--force-upload",
                    help="Force upload of records to REDCap even if a record of the "
                         "same name already exists.",
                    action="store_true")
parser.add_argument("--file-to-upload",
                    help="Absolute file path to txt file.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true",
                    default=False)
parser.add_argument("--only-converter-post-to-github", help="Do not double post issues to GitHub that are produced by commands called during conversion..", action="store_true", default=False)
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
parser.add_argument("--date",
                    help="If set then uploads all entries during that time period, e.g. {2017-06-29}:{2017-07-03} uploads everything that was in the svn repository from 2017-06-29 to 2017-07-03",
                    action="store",
                    default=None)

parser.add_argument("--last-week",
                    help="If set then uploads all entries during the last week", 
                    action="store_true",
                    default=False)

parser.add_argument("--all",
                    help="If set then uploads all entries", 
                    action="store_true",
                    default=False)

parser.add_argument("--dry-run",
                    help="Do not convert any data", 
                    action="store_true",
                    default=False)


parser.add_argument("--no-blaise",
                    help="Blaise files are going to be ignored", 
                    action="store_true",
                    default=False)

parser.add_argument("--stroop-only",
                    help="Only upload stroop file - should only be done in combination with --date as otherwise other records are not uploaded in the future (unless --date is used without the --stroop-only)", 
                    action="store_true",
                    default=False)

parser.add_argument("--include-test-files",
                    help="Include files that are stored in directories of the type <subject-id>-T-..., <subject-id>-X-... <site>-9999*<Sex>-...",
                    action="store_true",
                    default=False)

parser.add_argument("--omit-svn-update",
                    help="Do not run svn update. It is set to true when script is called with --file-to-upload. Only explicitly set flag when debugging an issue",
                    action="store_true",
                    default=False)

parser.add_argument('--progress-bar', help="Show TQDM progress bar", action="store_true")

# NOTE: Although most of the harvester logic is abstracted into functions, note
#       that most functions rely on the `args` variable to be available from
#       the calling scope
args = parser.parse_args()

# Setup logging
slog.init_log(args.verbose, args.post_to_github,'NCANDA Import-Laptop: harvester message', 'harvester', args.time_log_dir)
slog.startTimer1()

session = sibispy.Session()
if not session.configure() :
    sys.exit()

svn_client = session.connect_server('svn_laptop', True)
if not svn_client: 
    sys.exit()


svndir = os.path.join(session.get_laptop_svn_dir())
outdir = os.path.join(session.get_laptop_imported_dir())

# Figure out where this script is, so we can get path to other scripts.
bindir = os.path.dirname(os.path.realpath(__file__))

sibis_config = session.get_operations_dir()
if not os.path.exists(os.path.join(sibis_config, 'special_cases.yml')): 
    raise IOError("Please ensure special_cases.yml file exists at: {}".format(sibis_config))

# load exceptions for QC check 
with open(os.path.join(sibis_config, 'special_cases.yml'), 'r') as fi:
    complete_file = yaml.safe_load(fi)
    harvster_setting = complete_file.get('harvester')
    fi.close()

if harvster_setting: 
    ignore_data = harvster_setting.get('ignore', {})
    infer_dag_since = harvster_setting.get('infer_dag_since', {})
    for laptop in infer_dag_since:
        try:
            infer_dag_since[laptop] = datetime.datetime.strptime(
                infer_dag_since[laptop],
                r'%Y-%m-%d')
        except ValueError:
            print(("Laptop %s has a DAG inference setting, but invalid value"
                  " %s; should be YYYY-MM-DD.").format(laptop,
                                                       infer_dag_since[laptop]))
            infer_dag_since[laptop] = None

    # Load destination paths that are verboten
    ignore_processed_tree = harvster_setting.get('ignore_processed_paths', {})
    # Make tree into a list of paths
    ignore_processed = flatten_path_dict(ignore_processed_tree, outdir)
    # Normalize the paths so that string comparison can be used
    ignore_processed = [os.path.normpath(item) for item in ignore_processed]
else:
    # FIXME: Should condition on args.verbose?
    print("Warning: harvester specific settings not defined!")
    ignore_data = dict()
    infer_dag_since = dict()
    ignore_processed = []

def run_converter(site, subject_label, command, verbose, infer_dag=False):
    """
    Conversion tool.

    :param site: str
    :param command: str
    :return:
    """
    if verbose:
        print("Running", ' '.join(command))

        
    if args.dry_run :
        if verbose : 
            print("Dry Run: No files were transferred to csv") 
        return 0 

    filesProcessed = 0 

    try : 
        added_files = subprocess.check_output(command).decode()
        if not len(added_files):
            if verbose : 
                print("No files were transferred to csv") 
            return filesProcessed 

        for fi in added_files.strip().split('\n'):
            if re.match(r'.*\.csv$', fi):
                if os.path.normpath(fi) in ignore_processed:
                    if verbose:
                        print("Destination {} is ignored per configuration.".format(fi))
                elif os.path.basename(fi).split('-')[0] in ['NOID', 'nan']: 
                    slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi).encode()).hexdigest()[0:6], 'converter_cmd created file that does not contain subject ID and thus cannot be uploaded to redcap',
                              file=str(fi),
                              converter_cmd=" ".join(command), 
                              harvester_cmd=" ".join(sys.argv)) 
                else:
                    try:
                        if verbose:
                            print("Importing", fi, "into REDCap")

                        command_array = [os.path.join(bindir, 'csv2redcap')]
                        if args.force_upload:
                            command_array += ['--force-update']
                        if args.post_to_github:
                            command_array += ["-p"]
                        if args.time_log_dir:
                            command_array += ["-t", args.time_log_dir]

                        # NOTE: As of April 2018, all LimeSurvey files are
                        # collected on a UCSD server and uploaded via a UCSD
                        # SVN account (ucsd49), which induces harvester to
                        # assume all such records' DAG should be UCSD. For
                        # those cases, we want csv2redcap to look into the file
                        # and select the DAG based on the relevant variable if
                        # possible, rather than use --data-access-group.
                        #
                        # Whether DAG should be inferred is based on the
                        # setting file, section harvester::infer_dag_since,
                        # which is read at the top of this file. The actual
                        # logic is executed at the bottom of this file, prior
                        # to the handle_file_update call.
                        if infer_dag:
                            command_array += ['--use-file-dag']
                        else:
                            command_array += ['--data-access-group', site]

                        # Finally, add the file to the command
                        command_array += [fi]

                        if verbose: 
                            print("Running: " + ' '.join(command_array))

                        subprocess.call(command_array)
                        filesProcessed += 1

                    except:
                        error = "Failed importing files into REDCap"
                        slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi).encode()).hexdigest()[0:6], error,
                                  file=str(fi),
                                  converter_cmd=" ".join(command), 
                                  harvester_cmd=" ".join(sys.argv)) 
            else:
                slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi).encode()).hexdigest()[0:6], 'NOT A CSV FILE',
                          file=str(fi),
                          converter_cmd=" ".join(command), 
                          harvester_cmd=" ".join(sys.argv)) 

    except Exception as emsg:
        if args.only_converter_post_to_github:
            slog.sibisLogging().info(subject_label + "-" + hashlib.sha1(str(emsg).encode()).hexdigest()[0:6], 'Failed to convert data into redcap conform csv file',
                  converter_cmd=" ".join(command), 
                  harvester_cmd=" ".join(sys.argv), 
                  err_msg = str(emsg), trace=emsg)
        else :
            slog.info(subject_label + "-" + hashlib.sha1(str(emsg).encode()).hexdigest()[0:6], 'Failed to convert data into redcap conform csv file',
                  converter_cmd=" ".join(command), 
                  harvester_cmd=" ".join(sys.argv), 
                  err_msg = str(emsg), trace=emsg)

    return filesProcessed
#
# Function: hand file to correct converter
#
def handle_file(path, site, filename, verbose, infer_dag=False):
    # NOTE: infer_dag is only passed along for LimeSurvey files
    # Prepare option for overwriting
    if args.overwrite:
        overwrite = ["--overwrite"]
    else:
        overwrite = []

    if args.post_to_github or args.only_converter_post_to_github:
        post_to_github = ["--post-to-github"]
    else:
        post_to_github = []

    subject_label = path.split('/')[-2]
    # Is this a LimeSurvey file?
    if re.match(r'^survey.*\.csv$', filename ):
        # Never post to github as ou otherwise get two error messages - one from harvester and one from lime2csv 
        run_converter( site, subject_label, [ os.path.join( bindir, "lime2csv" ) ] + overwrite + post_to_github + [ path, os.path.join(outdir, site, "limesurvey" ) ], verbose, infer_dag=infer_dag)
    # Is this a Stroop file (Note: the "_100SD-" is signifigant as some MRI
    # Stroop files will include "_100SDMirror" in the filename)?
    elif re.match(r'^NCANDAStroopMtS_3cycles_7m53stask_100SD-[^/]*\.txt$', filename):
        filesProcessed = run_converter(site, subject_label, [os.path.join( bindir, "stroop2csv")] + overwrite +  post_to_github + [path, os.path.join(outdir, site, "stroop")], verbose)
        # Only perform eprime2 redcap if anything was processed 
        if filesProcessed > 0 : 
            eprime_cmd = [os.path.join(bindir, "eprime2redcap"), path, 'stroop_log_file']
            if verbose:
                print("Running", ' '.join(eprime_cmd))
                
            try:
                subprocess.check_output(eprime_cmd)
            except:
                slog.info(subject_label, 
                          "ERROR: could not upload Stroop file",
                          filename=filename,
                          converter_cmd=" ".join(eprime_cmd), 
                          harvester_cmd=" ".join(sys.argv))
    elif args.stroop_only : 
        return
    # Is this a Delayed Discounting file?
    # ignoes the V12-All.txt file 
    elif re.match(r'.*V12\.txt$', filename ):
        run_converter( site, subject_label, [ os.path.join( bindir, "dd2csv" ) ] + overwrite + post_to_github + [ path, os.path.join( outdir, site, "deldisc" ) ], verbose )
    # Is this a PASAT (Access) database?
    elif re.match(r'^PASAT_Stnd.*\.mdb$', filename ):
        run_converter( site, subject_label, [ os.path.join( bindir, "pasat2csv" ) ] + overwrite + [ path, os.path.join( outdir, site, "pasat" ) ], verbose )
    # Is this a SSAGA (Blaise) database?
    elif re.match(r'^NSSAGA_v3\.bdb$', filename ) or re.match(r'.*\.[Aa][Ss][Cc]$', filename ):
        if args.no_blaise :
            return 

        command_array = [os.path.join(bindir, 'wine/blaise2csv')] + overwrite + post_to_github
        if args.time_log_dir:
            command_array += ["-t", args.time_log_dir]
        if 'Youth_SAAGAv3' in path:
            command_array += [ path, 'youth', os.path.join( outdir, site, "ssaga" ) ]
        elif 'Parent_SAAGAv3' in path:
            command_array += [ path, 'parent', os.path.join( outdir, site, "ssaga" ) ]
        else:
            slog.info(subject_label, 'ERROR: could not determine whether the path contains Youth or Parent SSAGA',
                      path=str(path))
        run_converter(site, subject_label, command_array, verbose)

    elif verbose : 
        print("Warning: No conversion for file found!")

#
# Function: handle updated file by dispatching to the correct subhandler
#
def handle_file_update(path, verbose, infer_dag=False):
    # FIXME: For special DAG handling, should take an additional
    # argument to pass along to handle_file
    # First, let's get the site ID from the path
    match_site = re.search(r'ncanda/([A-Za-z]*)[^/]*/(.*)', path )

    if match_site:
        # Get the site ID
        site = match_site.group( 1 )
        # We do not accept data from the "admin" machines - testing only, not a collection site
        if site == 'admin':
            return

        filename = re.search(r'(.*)/([^/].*)', path ).group( 2 );
        handle_file(path, site, filename, verbose, infer_dag=infer_dag)
                            

#
# Main function: perform svn update and catch all resulting events
#


# If you only want to upload a specific file then do not run svn update
if args.file_to_upload: 
    args.omit_svn_update=True

if not args.omit_svn_update: 
    if args.verbose :
        print("Run svn update ...")
 
    svn_info = svn_client.info()

    try : 
        svn_updates = svn_client.update()
    except Exception as err_msg:
        print("ERROR: harvester failed to perform svn update !")
        (svn_list, err_msg) = session.__list_running_process__("[s]vn")
        # Make sure no other svn processs are running and that is while it is failing 
        print("INFO:svn processes running:", svn_list.decode("utf-8"))
        print(str(err_msg))
        sys.exit(1) 
         
    if len(svn_updates.errors) > 0:
        slog.info(hashlib.sha1(('harvester' + str(svn_updates.errors)).encode()).hexdigest()[0:6],
                  "ERROR: svn update returned errors !",
                  script='harvester',
                  msg=str(svn_updates.errors))
        sys.exit(1)

    if args.verbose :
        print("... done")
    
    svn_diff = svn_client.diff_path(revision=svn_info['entry_revision'])

    for changed in svn_diff.files_changed():
        updated_files.append(os.path.join(svndir, changed))
        
    if args.verbose :
        print("... done ") 


elif args.verbose: 
    print("Skip svn update!")

dated_download = "" 
if args.date : 
    dated_download = args.date

if args.all : 
    date_tomorrow = datetime.date.today() + datetime.timedelta(days=1)
    date_beginning = datetime.date(2012,0o1,0o1)
    dated_download =  "{" + str(date_beginning) + "}:{" + str(date_tomorrow) + "}"

if args.last_week : 
    date_tomorrow = datetime.date.today() + datetime.timedelta(days=1)
    date_last_week = date_tomorrow - datetime.timedelta(weeks=1)
    dated_download =  "{" + str(date_last_week) + "}:{" + str(date_tomorrow) + "}"

if dated_download : 
    # Files that are added and deleted during the time period are now listed !
    try:
        diff = svn_client.diff_path(revision=dated_download)
        for path in diff.files_changed(include_deleted=False):
            updated_files.append(os.path.join(svndir,path))

    except SibisSvnException as ex:
        slog.info(hashlib.sha1(('harvester' + str(ex)).encode()).hexdigest()[0:6],
                  "ERROR: Failed to update files in range " + args.date + ". Continued without them!",
                  script='harvester',
                  msg=str(ex))            
        
elif args.file_to_upload:
    if not os.path.exists(args.file_to_upload):
        slog.info(hashlib.sha1(('harvester'+args.file_to_upload).encode()).hexdigest()[0:6],
                  "ERROR: File does not exist!",
                  file=args.file_to_upload,
                  harvester_cmd=" ".join(sys.argv))
        sys.exit(1)

    updated_files.append(args.file_to_upload)

if args.verbose :
    num_files= len(updated_files)
    if num_files : 
        print("Files to be uploaded :\n" + "\n".join(updated_files))

    print("Number of files to be uploaded: ", num_files)
 
# Append single file to upload

# Process all updated or added files
# Equivalent to for file in updated_files, but with optional progress bar
for file in tqdm(updated_files, unit="files", disable=not args.progress_bar):
    # Test if excemption is set 
    if file.startswith(svndir):
        check_file = file[len(svndir):]
        if check_file[0] == '/' or check_file[0] == '\\' :
            check_file = check_file[1:]
    else : 
        check_file = file

    (cDir,cFile) = os.path.split(check_file)
    (laptop,visit) = os.path.split(cDir)
    if not args.include_test_files: 
        # check if it is a test file 
        if args.verbose:
            print(visit)
        visit_comp = visit.split('-')
        if len(visit_comp) > 1: 
            sub_num = visit_comp[1]
            sex =  visit_comp[2]
            if sex == 'T' or  sex == 'X' or sub_num == '99997' or sub_num == '99998' or sub_num == '99999' :
                if args.verbose : 
                    print("Skipping ", file)
                continue 

    if laptop in iter(ignore_data.keys()):
        ignore_laptop=ignore_data[laptop].split(',')  
        if os.path.join(visit,cFile) in ignore_laptop :
            if args.verbose : 
                print("Skipping ", file)
            continue 
 
    # Check if DAG should be derived from the file path, or inferred from the
    # file itself (much later, at csv2redcap invocation)
    infer_dag = False
    if laptop in infer_dag_since:
        infer_dag_start = infer_dag_since[laptop]
        if infer_dag_start:
            # check if file's Last Modified is after the DAG inference start
            last_modified = datetime.datetime.fromtimestamp(
                os.path.getmtime(file))
            if infer_dag_start <= last_modified:
                infer_dag = True
        else:
            # FIXME: Should this fail instead because the value *should* be a date?
            infer_dag = True

    handle_file_update(file, args.verbose, infer_dag=infer_dag)

slog.takeTimer1("script_time","{'records': " + str(len(updated_files)) + "}")


