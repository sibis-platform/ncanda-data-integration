#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
import os
import argparse
import cnp
import datetime
import numpy as np
import pandas
import sys 
import hashlib
import numbers


import sibispy
from sibispy import sibislogger as slog

# Convert to string, or empty if nan.
def nan_to_empty( x ):
    s = str(x)
    if s != 'nan':
        return s
    else:
        return ''

def df_rounding(row): 
    for ind in row.index:
        if isinstance(row[ind], numbers.Number): 
            row[ind]= round(row[ind],6) 
    return row

# Setup command line parser
parser = argparse.ArgumentParser( description="Update WebCNP summary forms from imported data", formatter_class=argparse.ArgumentDefaultsHelpFormatter )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "--max-days-after-visit", help="Maximum number of days the scan session can be after the entered 'visit date' in REDCap to be assigned to a given event.", action="store", default=120, type=int)
parser.add_argument( "-a", "--update-all", help="Update all summary records, regardless of current completion status (otherwise, only update records where incoming data completion status exceeds existing summary data status)",
                     action="store_true")
parser.add_argument( "--records-per-upload", help="Maximum number of records to upload to REDCap using a single HTTP request. This limits the request size and prevents upload problems.", action="store", default=30, type=int)
parser.add_argument( "-n", "--no-upload", help="Do not upload any data to REDCap server; instead write to CSV file with given path.", action="store")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
parser.add_argument( "--study-id", help="Limit export by subject site id (e.g., 'X-12345-X-9').", action="store", default=None )
args = parser.parse_args()

# Open connection with REDCap server - first for the Penn import project (data source)
slog.init_log(args.verbose, args.post_to_github,'NCANDA Import', 'update_summary_forms', args.time_log_dir)
slog.startTimer1()

session = sibispy.Session()
if not session.configure():
    if args.verbose:
        print "Error: session configure file was not found"

    sys.exit()

rc_import = session.connect_server('import_webcnp', True)
if not rc_import : 
    if args.verbose:
        print "Error: Could not connect to the REDCap project 'import_webcnp'" 

    sys.exit(1)


# Second connection for the Summary project (this is where we put data)
rc_summary = session.connect_server('data_entry', True)
if not rc_summary : 
    if args.verbose:
        print "Error: Could not connect to the REDCap project 'data_entry'" 

    sys.exit(1)

# Get list of variables common to import and summary projects (minus 'cnp_' prefix in the latter)
cnp_copy_variables = cnp.get_copy_variables( rc_import, rc_summary )

# Get the event-form mapping so we can select the events that should have CNP data
form_event_mapping = rc_summary.export_fem( format='df' )
form_key = session.get_redcap_form_key()
cnp_events_list = form_event_mapping[form_event_mapping[form_key] == 'cnp_summary' ]['unique_event_name'].tolist()

# Retrieve subject IDs, events, exclusions, visit dates, and current CNP summary data
fields_summary = ['study_id', 'redcap_event_name', 'dob', 'exclude', 'visit_date', 'cnp_summary_complete', 'cnp_missing', 'cnp_datasetid', 'cnp_age', 'cnp_instruments']
fields_summary+= [ 'cnp_%s' % v for v in cnp_copy_variables ]
fields_summary+= [ 'cnp_%s_zscore' % v for v in cnp.mean_sdev_by_field_dict.keys() ]
baseline_events = ['baseline_visit_arm_1','baseline_visit_arm_4'] # these are the "baseline" events of the study arms that have them

summary_records = rc_summary.export_records( fields=fields_summary, event_name='unique', format='df')
# Drop all records that don't have "visit_date", i.e., all events from arms other than Arm 1 (Standard Protocol)
summary_records = summary_records[ summary_records['visit_date'].map( lambda x: str(x) != 'nan' ) ]

# Now drop all records from events that don't have CNP data collected
summary_records = summary_records[ summary_records.index.map( lambda x: x[1] in cnp_events_list ) ]

# Now select ids 
if args.study_id : 
    if args.study_id in summary_records.index : 
        summary_records = summary_records.xs(args.study_id)
        summary_records.reset_index(inplace=True)
        summary_records['study_id'] = args.study_id

        baseline_events_tmp = [] 
        for event in baseline_events :
            if event in summary_records['redcap_event_name'].unique() :
                baseline_events_tmp += [event] 

        baseline_events = baseline_events_tmp 

        summary_records.set_index(['study_id', 'redcap_event_name'],inplace=True)

    else :
        print "ERROR: No record found for",  args.study_id 
        sys.exit(1)

# Have to do it as otherwise int values are changed to float if NA included in colums and that creates problems  
summary_records['cnp_missing'] = summary_records['cnp_missing'].fillna(0).astype(int)

# Save dates of birth for later (these only exist for the 'baseline_visit_arm_1' and  'baseline_visit_arm_4' events, but we need them for other arms as well
subject_dates_of_birth = pandas.concat( [ summary_records.xs( event, level=1 ) for event in baseline_events ] )['dob']

# Convert all copied columns to strings to avoid issues with missing values being unable to convert to floats
summary_records['cnp_datasetid'] = summary_records['cnp_datasetid'].map( nan_to_empty )
summary_records['cnp_summary_complete'] = summary_records['cnp_summary_complete'].map( nan_to_empty )
for cnpvar in cnp_copy_variables:
    summary_records['cnp_%s' % cnpvar] = summary_records['cnp_%s' % cnpvar].map( str )

# If we update all records, make sure to reset everything, so stuff can be made disappear in the summary, if it was removed from the imported project
if args.update_all:
    summary_records['cnp_summary_complete'] = ''
    summary_records['cnp_datasetid'] = ''

    for cnpvar in cnp_copy_variables:
        summary_records['cnp_%s' % cnpvar] = ''

    for cnpvar in cnp.mean_sdev_by_field_dict.keys():
        summary_records['cnp_%s_zscore' % cnpvar] = ''

    for [k,v] in cnp.instruments.iteritems():
        summary_records['cnp_instruments___%s' % k.replace('_','') ] = '0'
else:
    summary_records = summary_records[ summary_records['cnp_datasetid'] == '' ]



# For now, get the "complete" fields for each instrument as well as record ID, subject ID, and date of test
fields_imported = ['record_id', 'test_sessions_subid'] + cnp_copy_variables
for i in cnp.instruments.values():
    fields_imported.append( '%s_complete' % i )
imported_records = rc_import.export_records( fields=fields_imported, format='df')
for cnpvar in cnp_copy_variables:
    imported_records[cnpvar] = imported_records[cnpvar].map( nan_to_empty )

# From the index, extract the original subject ID and date from each imported record IO and use these wherever not overridden by manually-entered values
imported_records['test_sessions_subid'] = imported_records['test_sessions_subid'].map( nan_to_empty )
imported_records['test_sessions_subid'] = imported_records.apply( lambda row: row['test_sessions_subid'] if row['test_sessions_subid']!='' else row.name[0:-17], axis=1 )

imported_records['test_sessions_dotest'] = imported_records['test_sessions_dotest'].map( nan_to_empty )
imported_records['test_sessions_dotest'] = imported_records.apply( lambda row: row['test_sessions_dotest'] if row['test_sessions_dotest']!='' else row.name[-16:-6], axis=1 )

#    
# Go over all summary records (i.e., the visit log) and find corresponding imported records
#
for key, row in summary_records.iterrows():
    # Select imported records for this subject
    records_this_subject = imported_records[ imported_records['test_sessions_subid'] == key[0] ]

    # Get the visit date for this record
    visit_date = row['visit_date']
    if not str(visit_date) == 'nan':
        # Select all records within given maximum number of days after visit date
        records_this_visit = records_this_subject[ records_this_subject['test_sessions_dotest'] >= visit_date ]
        visit_date_plusNd = (datetime.datetime.strptime( visit_date, '%Y-%m-%d') + datetime.timedelta(args.max_days_after_visit)).strftime('%Y-%m-%d')
        records_this_visit = records_this_visit[ records_this_visit['test_sessions_dotest'] <= visit_date_plusNd ]

        if len( records_this_visit ) > 0:
            # Make sure there is only one, unique record
            if len( records_this_visit ) > 1:
                # Not unique - warning
                error = 'WARNING: More than one CNP record found for subject %s, event %s, visit date %s - selecting the latest record' % (key[0],key[1],visit_date)
                slog.info( "DuplicateRecords-" + hashlib.sha1(error).hexdigest()[0:6], error,
                           records_this_visit = str(records_this_visit.index.tolist()))

            # Unique record - copy data from imported project to summary form
            cnp_data = records_this_visit.ix[ len( records_this_visit ) - 1 ]
            summary_records.at[key, 'cnp_datasetid'] = cnp_data.name

            date_of_birth = subject_dates_of_birth[key[0]]
            date_format_ymd = '%Y-%m-%d'
            age_in_years = (datetime.datetime.strptime( cnp_data['test_sessions_dotest'], date_format_ymd ) - datetime.datetime.strptime( date_of_birth, date_format_ymd )).days / 365.242
            summary_records.at[key, 'cnp_age'] = str( age_in_years )

            # Copy all variables that we want in the summary
            for cnpvar in cnp_copy_variables:
                column_name = 'cnp_%s' % cnpvar
                summary_records.at[key, column_name] = cnp_data[cnpvar]

            # Compute all z scores (only between 8 and 21 years)
            if age_in_years >= 8 and age_in_years < 22:
                age_group = int( age_in_years / 2 ) * 2
                for cnpvar in cnp.mean_sdev_by_field_dict.keys():
                    if cnp_data[cnpvar] != '':
                        mean_sdev_col_label = cnp.mean_sdev_by_field_dict[cnpvar]
                        age_mean = cnp.mean_sdev_byage_table['%s_mean' % mean_sdev_col_label][age_group]
                        age_sdev = cnp.mean_sdev_byage_table['%s_sd' % mean_sdev_col_label][age_group]
                        age_normalization = ( float( cnp_data[cnpvar] ) - age_mean ) / age_sdev
                        column_name = 'cnp_%s_zscore' % cnpvar
                        summary_records.at[key, column_name] = age_normalization

            # Check completion status of the CNP instruments
            for [k,v] in cnp.instruments.iteritems():
                summary_complete = 1
                if cnp_data['%s_complete' % v] > 0:
                    column_name = 'cnp_instruments___%s' % k.replace('_','')
                    summary_records.at[key, column_name] = '1'
                else:
                    column_name = 'cnp_instruments___%s' % k.replace('_','')
                    summary_records.at[key, column_name] = '0'
                    summary_complete = 0

            summary_records.at[key, 'cnp_summary_complete'] = summary_complete
        elif summary_records['cnp_summary_complete'][key] != '' and float( summary_records['cnp_summary_complete'][key] ) > 0 and ( summary_records['cnp_missing'][key] != 1 ):
            print "WARNING: Previously assigned WebCNP data for subject",key[0],"event",key[1],"appears to have disappeared."

# Drop all summary records for which there is no CNP data
summary_records = summary_records[ summary_records['cnp_datasetid'] != '' ]

# Drop all (logically) read-only columns from the summary data - these are the ones that are neither index-related (subject, event), nor part of the data imported from WebCNP
summary_records = summary_records.drop( [ 'dob', 'exclude', 'visit_date' ], axis=1 )

uploaded_count = 0

# round up age and z-scores as records as otherwise creates problems when writing to redcap 
# summary_records=summary_records['cnp_age'].astype(float).round(6)
rnd_list = ['cnp_age']
for col_name in summary_records.columns.values:
    if col_name.endswith('_zscore') :
        rnd_list += [col_name] 

summary_records[rnd_list] = summary_records[rnd_list].apply(df_rounding, axis=1) 

        # col = summary_records[col_name]
        # Did not work as there was a string somewhere still
        # summary_records.loc[col.notnull(),col_name] = col[col.notnull()].astype(float).round(6)
        #apply(rounding, axis=1 )

if args.no_upload:
    if args.verbose:
        print "Writing",len( summary_records ),"records to",args.no_upload
    summary_records.to_csv( args.no_upload )
else:
    if args.verbose:
        print "Uploading",len(summary_records),"records."

    error_label = "ImportToDataEntry" 
    for from_index in range(0,len(summary_records),args.records_per_upload):
        # Upload next block of records of new data to REDCap
        to_index = min( from_index + args.records_per_upload, len(summary_records) )
        if args.verbose:
            print "Uploading records",from_index,"to",to_index-1
        
        import_response =  session.redcap_import_record(error_label, "", "", "UploadRecords", summary_records[from_index:to_index])
        if import_response :
            # Count uploaded records
            if 'count' in import_response.keys():
                uploaded_count += import_response['count']
            if 'error' in import_response.keys():
                slog.info(error_label + "-" + hashlib.sha1(str(import_response['error'])).hexdigest()[0:6], "ERROR: Uploading Data", error_msg = str(import_response['error']))
            if 'fields' in import_response.keys():
                slog.info(error_label + "-" + hashlib.sha1(str(import_response['fields'])).hexdigest()[0:6], "Info: something wrong with fields ! Not sure what to do !", fields =  str(import_response['fields']))
            if 'records' in import_response.keys():
                slog.info(error_label + "-" + hashlib.sha1(str(import_response['records'])).hexdigest()[0:6], "Info: something wrong with redcords ! Not sure what to do !", records =  str(import_response['records']))

    # Finally, print upload status if so desired                    
    if args.verbose:
        print "Successfully uploaded %d/%d records to REDCap." % ( uploaded_count, len( summary_records ) )

slog.takeTimer1("script_time","{'records': " + str(len(summary_records)) + ", 'uploads': " +  str(uploaded_count) + "}")
